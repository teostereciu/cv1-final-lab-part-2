{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://miro.medium.com/v2/resize:fit:1200/1*lbDXL0IuitCRz4mpZ7MmfQ.png\" width=55% > </center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center> \n",
    "    <font size=\"6\">Final Lab (Part 2): Image Classification using Convolutional Neural Networks </font>\n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Computer Vision 1 University of Amsterdam</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Due 23:59PM, October 18, 2024 (Amsterdam time)</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\"><b>TA's:  Yue, Konrad & Thies</b></font>\n",
    "</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "\n",
    "Student1 ID:  15732592 \\\n",
    "Student1 Name: Marko Ivanov\n",
    "\n",
    "Student2 ID: 15743063 \\\n",
    "Student2 Name: Teodora Stereciu\n",
    "\n",
    "Student3 ID: 14836459 \\\n",
    "Student3 Name: Ranjan Mishra\n",
    "\n",
    "Student4 ID: 15494349 \\\n",
    "Student4 Name: Isabelle Kampono\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Coding Guidelines**\n",
    "\n",
    "Your code must be handed in this Jupyter notebook, renamed to **StudentID1_StudentID2_StudentID3.ipynb** before the deadline by submitting it to the Canvas Final Lab: Image Classification Assignment. Please also fill out your names and IDs above.\n",
    "\n",
    "For full credit, make sure your notebook follows these guidelines:\n",
    "\n",
    "- Please express your thoughts **concisely**. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Understand the problem as much as you can. When answering a question, provide evidence (qualitative and/or quantitative results, references to papers, figures, etc.) to support your arguments. Not everything might be explicitly asked for, so think about what might strengthen your arguments to make the notebook self-contained and complete.\n",
    "- Tables and figures must be accompanied by a **brief** description. Add a number, a title, and, if applicable, the name and unit of variables in a table, and name and unit of axes and legends in a figure.\n",
    "\n",
    "**Late submissions are not allowed.** Assignments submitted after the strict deadline will not be graded. In case of submission conflicts, TAsâ€™ system clock is taken as reference. We strongly recommend submitting well in advance to avoid last-minute system failure issues.\n",
    "\n",
    "**Environment:** Since this is a project-based assignment, you are free to use any feature descriptor and machine learning tools (e.g., K-means, SVM). You should use Python for your implementation. You are free to use any Python library for this assignment, but make sure to provide a conda environment file!\n",
    "\n",
    "**Plagiarism Note:** Keep in mind that plagiarism (submitted materials which are not your work) is a serious offense and any misconduct will be addressed according to university regulations. This includes using generative tools such as ChatGPT.\n",
    "\n",
    "**Ensure that you save all results/answers to the questions (even if you reuse some code).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Report Preparation**\n",
    "\n",
    "Your tasks include the following:\n",
    "\n",
    "1. **Report Preparation:** For both parts of the final project, students are expected to prepare a report. The report should include all details on implementation approaches, analysis of results for different settings, and visualizations illustrating experiments and performance of your implementation. Grading will be based on the report, so it should be as self-contained as possible. If the report contains faulty results or ambiguities, TAs can refer to your code for clarification. \n",
    "\n",
    "2. **Explanation of Results:** Do not just provide numbers without explanation. Discuss different settings to show your understanding of the material and processes involved.\n",
    "\n",
    "3. **Quantitative Evaluation:** For quantitative evaluation, you are expected to provide the results based on performance (accuracy, learning loss and learning curves). \n",
    "\n",
    "4. **Aim:** Understand the basic Image Classification pipeline using Convolutional Neural Nets (CNN's).\n",
    "\n",
    "5. **Working on Assignments:** Students should work in assigned groups for **two** weeks. Any questions can be discussed on ED.\n",
    "\n",
    "    - **Submission:** Submit your source code and report together in a zip file (`ID1_ID2_ID3_part2.zip`). The report should be a maximum of 10 pages (single-column, including tables and figures, excluding references and appendix). Express thoughts concisely. Tables and figures must be accompanied by a description. Number them and, if applicable, name variables in tables, and label axes in figures.\n",
    "\n",
    "6. **Hyperparameter Search:** In your experiments, remember to perform a hyperparameter search to find the optimal settings for your model(s). Clearly document the search process, the parameters you explored, and how they influenced the performance of your model.\n",
    "\n",
    "8. **Format and Testing:** The report should be in **PDF format**, and the code in **.ipynb format**. Test that all functionality works as expected in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "\n",
    "- [Section 1: Image Classification on CIFAR-100 (0 points)](#section-1)\n",
    "- [Section 2: Visualizing CIFAR-100 Classes and Subclasses (3 points)](#section-2)\n",
    "- [Section 3: TwoLayerNet Architecture (2 points)](#section-3)\n",
    "- [Section 4: ConvNet Architecture (2 points)](#section-4)\n",
    "- [Section 5: Preparation of Training (7 points)](#section-5)\n",
    "- [Section 6: Training the Networks (5 points)](#section-6)\n",
    "- [Section 7: Setting Up the Hyperparameters (14 points)](#section-7)\n",
    "- [Section 8: Visualizing the STL-10 Dataset and Preparing the Data Loader (3 points)](#section-8)\n",
    "- [Section 9: Fine-tuning ConvNet on STL-10 (14 points)](#section-9)\n",
    "- [Section 10: Bonus Challenge (optional)](#section-10)\n",
    "- [Section X: Individual Contribution Report (Mandatory)](#section-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Section 1: Image Classification on CIFAR-100 (0 points)**\n",
    "\n",
    "The goal of this lab is to implement an image classification system using Convolutional Neural Networks (CNNs) that can identify objects from a set of classes in the [CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). You will implement and compare two different architectures: a simple two-layer network and a ConvNet based on the LeNet architecture.\n",
    "\n",
    "The CIFAR-100 dataset contains 32x32 pixel RGB images, categorized into 100 different classes. The dataset will be automatically downloaded and loaded using the code provided in this notebook.\n",
    "\n",
    "You will train and test your classification system using the entire CIFAR-100 dataset. Ensure that the test images are excluded from training to maintain a fair evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Data loaders for CIFAR-100 are ready for use.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import optuna\n",
    "\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "SEED = 1810\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "])\n",
    "\n",
    "# Load the CIFAR-100 training set\n",
    "train_set = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load the CIFAR-100 test set\n",
    "test_set = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders for the entire CIFAR-100 dataset\n",
    "train_data_loader = DataLoader(train_set, shuffle=True)\n",
    "test_data_loader = DataLoader(test_set, shuffle=False)\n",
    "\n",
    "# Define CIFAR-100 superclasses and their subclasses\n",
    "superclasses = {\n",
    "    'aquatic mammals': ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
    "    'fish': ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
    "    'flowers': ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
    "    'food containers': ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
    "    'fruit and vegetables': ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],\n",
    "    'household electrical devices': ['clock', 'keyboard', 'lamp', 'telephone', 'television'],\n",
    "    'household furniture': ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
    "    'insects': ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
    "    'large carnivores': ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
    "    'large man-made outdoor things': ['bridge', 'castle', 'house', 'road', 'skyscraper'],\n",
    "    'large natural outdoor scenes': ['cloud', 'forest', 'mountain', 'plain', 'sea'],\n",
    "    'large omnivores and herbivores': ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],\n",
    "    'medium-sized mammals': ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
    "    'non-insect invertebrates': ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
    "    'people': ['baby', 'boy', 'girl', 'man', 'woman'],\n",
    "    'reptiles': ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
    "    'small mammals': ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
    "    'trees': ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],\n",
    "    'vehicles 1': ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n",
    "    'vehicles 2': ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']\n",
    "}\n",
    "\n",
    "# List of all CIFAR-100 classes\n",
    "classes = ('apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', \n",
    "           'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle',\n",
    "           'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
    "           'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard',\n",
    "           'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain',\n",
    "           'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree',\n",
    "           'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea',\n",
    "           'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', \n",
    "           'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', \n",
    "           'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm')\n",
    "\n",
    "# Create a mapping of class names to their indices\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "\n",
    "# Create a mapping of superclasses to their corresponding class indices\n",
    "superclass_to_indices = {supcls: [class_to_idx[cls] for cls in subclasses] for supcls, subclasses in superclasses.items()}\n",
    "\n",
    "print(\"Data loaders for CIFAR-100 are ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-2\"></a>\n",
    "### **Section 2: Visualizing CIFAR-100 Classes and Subclasses (3 points)**\n",
    "\n",
    "In this section, you will implement a function to visualize the CIFAR-100 dataset, including **all** superclasses and their corresponding subclasses. Your implementation should provide a clear and organized overview of the dataset's diversity.\n",
    "\n",
    "You add the figure(s) to appendix of your report and refer to it in the main text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# function to denormalize images for plotting\n",
    "def denormalize(image):\n",
    "    image = image / 2 + 0.5  \n",
    "    np_image = image.numpy()  \n",
    "    return np.transpose(np_image, (1, 2, 0))  # rearrange dimensions to HWC\n",
    "\n",
    "# split superclasses into two groups for two columns\n",
    "def split_superclasses(superclass_dict):\n",
    "    superclasses_list = list(superclass_dict.items())\n",
    "    mid = len(superclasses_list) // 2\n",
    "    return superclasses_list[:mid], superclasses_list[mid:]\n",
    "\n",
    "# cache to store one preselected image for each subclass\n",
    "preselected_images = {}\n",
    "\n",
    "# use a DataLoader without shuffling for reproducibility\n",
    "fixed_data_loader = DataLoader(train_set, shuffle=False, batch_size=1)\n",
    "\n",
    "# function to cache one image for each subclass deterministically\n",
    "def cache_images_for_subclasses():\n",
    "    for data, target in fixed_data_loader:\n",
    "        label = target.item()\n",
    "        if label not in preselected_images:\n",
    "            preselected_images[label] = data[0]  # store the first image for each subclass\n",
    "        if len(preselected_images) == len(classes):  # stop when all subclasses have an image cached\n",
    "            break\n",
    "\n",
    "# function to cache one image for each subclass\n",
    "def cache_images_for_subclasses():\n",
    "    for data, target in train_data_loader:\n",
    "        for img, label in zip(data, target):\n",
    "            if label.item() not in preselected_images:\n",
    "                preselected_images[label.item()] = img\n",
    "            if len(preselected_images) == len(classes):  # stop when all subclasses have an image\n",
    "                return\n",
    "\n",
    "cache_images_for_subclasses()\n",
    "\n",
    "# function to visualize superclasses and subclasses with images and labels on top\n",
    "def visualize_cifar100_superclasses_all_subclasses(superclass_to_indices):\n",
    "    # split superclasses into two columns\n",
    "    col1_superclasses, col2_superclasses = split_superclasses(superclass_to_indices)\n",
    "\n",
    "    num_superclasses = len(col1_superclasses) \n",
    "    max_subclasses = max([len(indices) for _, indices in superclass_to_indices.items()]) \n",
    "    \n",
    "    fig, axs = plt.subplots(num_superclasses, 2 * max_subclasses + 2, figsize=(40, 3 * num_superclasses), constrained_layout=True)\n",
    "\n",
    "    fig.suptitle(\"Superclasses and Corresponding Subclasses for CIFAR-100\", fontsize=24, fontweight='bold', y=1.03)\n",
    "\n",
    "    # helper function to plot one side (column) of superclasses\n",
    "    def plot_column(superclasses, col_start):\n",
    "        for i, (superclass, indices) in enumerate(superclasses):\n",
    "            row = i\n",
    "            axs[row, col_start].text(0.5, 0.5, superclass, fontsize=18, fontweight='bold', ha='center', va='center')\n",
    "            axs[row, col_start].axis('off') \n",
    "            count = 0\n",
    "            for subclass_idx in indices:\n",
    "                img = preselected_images[subclass_idx] \n",
    "                axs[row, col_start + count + 1].imshow(denormalize(img))\n",
    "                axs[row, col_start + count + 1].set_title(classes[subclass_idx], fontsize=16) \n",
    "                axs[row, col_start + count + 1].axis('off')\n",
    "                count += 1\n",
    "\n",
    "    # plot left column\n",
    "    plot_column(col1_superclasses, 0)\n",
    "    \n",
    "    # plot right column\n",
    "    plot_column(col2_superclasses, max_subclasses + 1)\n",
    "    \n",
    "    plt.savefig(\"cifar100_visualization.png\", dpi=300) \n",
    "    plt.show()\n",
    "\n",
    "# display cifar100 images and save the figure\n",
    "visualize_cifar100_superclasses_all_subclasses(superclass_to_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3\"></a>\n",
    "### **Section 3: TwoLayerNet Architecture (2 points)**\n",
    "\n",
    "In this section, you will implement the architecture of a fully connected neural network called `TwoLayerNet`, consisting of two fully connected layers with a ReLU activation in between. The network accepts an input size of 3x32x32 (CIFAR-100 image), a specified hidden layer size, and the number of output classes. In the `__init__` method, define the first fully connected layer that maps the input size to the hidden size, and the second fully connected layer that maps the hidden size to the number of classes. \n",
    "\n",
    "Ensure to call the parent class constructor using `super(TwoLayerNet, self).__init__()`. In the `forward` method, flatten the input tensor, pass it through the first layer with ReLU activation, and then through the second layer to obtain the final scores.\n",
    "\n",
    "**Note:** You are allowed to modify the provided function definitions as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        '''\n",
    "        Initializes the two-layer neural network model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input features.\n",
    "            hidden_size (int): The size of the hidden layer.\n",
    "            num_classes (int): The number of classes in the dataset.\n",
    "        '''\n",
    "\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "\n",
    "        self.dense1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dense2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        '''\n",
    "\n",
    "        flat_x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        hidden = self.dense1(flat_x)\n",
    "        hidden = F.relu(hidden)\n",
    "        hidden = self.dense2(hidden)\n",
    "\n",
    "        output = F.log_softmax(hidden, dim=1)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-4\"></a>\n",
    "### **Section 4: ConvNet Architecture (2 points)**\n",
    "\n",
    "In this section, you will implement a convolutional neural network inspired by the structure of [LeNet-5](https://ieeexplore.ieee.org/document/726791). The network processes color images using three convolutional layers followed by two fully connected layers. Since you need to feed color images into this network, determine the kernel size of the first convolutional layer. Additionally, calculate the number of trainable parameters in the \"F6\" layer, providing the calculation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, num_classes=100):\n",
    "        '''\t\n",
    "        Initializes the convolutional neural network model.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 6*in_channels, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(6*in_channels, 16*in_channels, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5*in_channels, 120*in_channels),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(120*in_channels, 84*in_channels),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(84*in_channels, num_classes),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        '''\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        probas = F.log_softmax(logits, dim=1)\n",
    "        return probas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-5\"></a>\n",
    "### **Section 5: Preparation of Training (7 points)**\n",
    "\n",
    "In this section, you will create a custom dataset class to load the CIFAR-100 data, define a transform function for data augmentation, and set up an optimizer for training. While the previous section utilized the built-in CIFAR-100 class from `torchvision`, in practice, you often need to prepare datasets manually. Here, you will implement the `CIFAR100_loader` class to handle the dataset and use `DataLoader` to make it iterable. You will also define a transform function for data augmentation and an optimizer for updating the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR100_loader(Dataset):\n",
    "    \n",
    "    def __init__(self, root, train=True, transform=None, download=False):\n",
    "        '''\n",
    "        Initializes the CIFAR-100 dataset loader.\n",
    "\n",
    "        Args:\n",
    "            root (str): The root directory to store the dataset.\n",
    "            train (bool): If True, loads the training data; otherwise, loads the test data.\n",
    "            transform (callable, optional): The data transformations to apply.\n",
    "            download (bool): If True, downloads the dataset if it is not already available.\n",
    "        '''\n",
    "    \n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load the CIFAR-100 dataset from torchvision\n",
    "        self.dataset = torchvision.datasets.CIFAR100(root=self.root, train=self.train, \n",
    "                                                     transform=self.transform, download=download)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of samples in the dataset.\n",
    "        '''\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Retrieves a sample from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the image and label tensors.\n",
    "        '''\n",
    "\n",
    "        img, label = self.dataset[idx] \n",
    "        \n",
    "        # apply transformation (if any)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transforms():\n",
    "    '''\n",
    "    Creates the data transformations for the CIFAR-100 dataset.\n",
    "\n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: The data transformations for the dataset.\n",
    "    '''\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1] range\n",
    "    ])\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, learning_rate=0.001):\n",
    "    '''\n",
    "    Creates an optimizer for the model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        learning_rate (float): The learning rate for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        torch.optim.Adam: The optimizer for the model.\n",
    "    '''\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-6\"></a>\n",
    "### **Section 6: Training the Networks (5 points)**\n",
    "\n",
    "In this section, you will complete the `train` function and use it to train both the `TwoLayerNet` and `ConvNet` models. You will use the custom `CIFAR100_loader`, transform function, and optimizer function that you implemented. The goal is to compare the performance of the two models on the CIFAR-100 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, testloader):\n",
    "    '''\n",
    "    Validates the model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network model.\n",
    "        testloader (torch.utils.data.DataLoader): The data loader for the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the test dataset.\n",
    "    '''\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Iterate over the test dataset\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f} %')\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_per_class(net, testloader, classes):\n",
    "    '''\n",
    "    Validates the model on the test dataset per class.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network model.\n",
    "        testloader (torch.utils.data.DataLoader): The data loader for the test dataset.\n",
    "        classes (tuple): The tuple of class names.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    class_correct = [0. for _ in range(len(classes))]\n",
    "    class_total = [0. for _ in range(len(classes))]\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Iterate over the test dataset\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions = (predicted == labels).squeeze()\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += correct_predictions[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        accuracy = 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
    "        print(f'Accuracy of {class_name:5s} : {accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, criterion, optimizer, epochs=100):\n",
    "    '''\n",
    "    Trains the neural network model.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network model.\n",
    "        train_loader (torch.utils.data.DataLoader): The data loader for the training dataset.\n",
    "        criterion (torch.nn.Module): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for the model.\n",
    "        epochs (int): The number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    net.train()\n",
    "    net.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # iterate over batches of training data\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # print epoch summary\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialize the datasets and data loaders for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = create_transforms()\n",
    "\n",
    "# load CIFAR-100 dataset\n",
    "train_set = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# create train and test DataLoaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "input_size = 3 * 32 * 32\n",
    "hidden_size = 512        \n",
    "num_classes = 100\n",
    "\n",
    "# initialize MLP\n",
    "mlp = TwoLayerNet(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
    "\n",
    "# initialize CNN\n",
    "convnet = ConvNet(in_channels=3, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the TwoLayerNet model on the CIFAR-100 dataset using the training data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.8127\n",
      "Epoch [2/10], Loss: 3.4044\n",
      "Epoch [3/10], Loss: 3.1945\n",
      "Epoch [4/10], Loss: 3.0306\n",
      "Epoch [5/10], Loss: 2.8713\n",
      "Epoch [6/10], Loss: 2.7190\n",
      "Epoch [7/10], Loss: 2.5660\n",
      "Epoch [8/10], Loss: 2.4316\n",
      "Epoch [9/10], Loss: 2.3106\n",
      "Epoch [10/10], Loss: 2.1883\n",
      "Finished Training\n",
      "Accuracy of the network on the test images: 22.72 %\n",
      "Accuracy of apple : 44.00 %\n",
      "Accuracy of aquarium_fish : 40.00 %\n",
      "Accuracy of baby  : 8.00 %\n",
      "Accuracy of bear  : 14.00 %\n",
      "Accuracy of beaver : 4.00 %\n",
      "Accuracy of bed   : 23.00 %\n",
      "Accuracy of bee   : 30.00 %\n",
      "Accuracy of beetle : 14.00 %\n",
      "Accuracy of bicycle : 29.00 %\n",
      "Accuracy of bottle : 36.00 %\n",
      "Accuracy of bowl  : 17.00 %\n",
      "Accuracy of boy   : 5.00 %\n",
      "Accuracy of bridge : 18.00 %\n",
      "Accuracy of bus   : 20.00 %\n",
      "Accuracy of butterfly : 10.00 %\n",
      "Accuracy of camel : 24.00 %\n",
      "Accuracy of can   : 19.00 %\n",
      "Accuracy of castle : 37.00 %\n",
      "Accuracy of caterpillar : 13.00 %\n",
      "Accuracy of cattle : 7.00 %\n",
      "Accuracy of chair : 37.00 %\n",
      "Accuracy of chimpanzee : 47.00 %\n",
      "Accuracy of clock : 26.00 %\n",
      "Accuracy of cloud : 32.00 %\n",
      "Accuracy of cockroach : 42.00 %\n",
      "Accuracy of couch : 8.00 %\n",
      "Accuracy of crab  : 13.00 %\n",
      "Accuracy of crocodile : 15.00 %\n",
      "Accuracy of cup   : 36.00 %\n",
      "Accuracy of dinosaur : 4.00 %\n",
      "Accuracy of dolphin : 29.00 %\n",
      "Accuracy of elephant : 23.00 %\n",
      "Accuracy of flatfish : 18.00 %\n",
      "Accuracy of forest : 25.00 %\n",
      "Accuracy of fox   : 24.00 %\n",
      "Accuracy of girl  : 19.00 %\n",
      "Accuracy of hamster : 22.00 %\n",
      "Accuracy of house : 11.00 %\n",
      "Accuracy of kangaroo : 17.00 %\n",
      "Accuracy of keyboard : 14.00 %\n",
      "Accuracy of lamp  : 31.00 %\n",
      "Accuracy of lawn_mower : 24.00 %\n",
      "Accuracy of leopard : 4.00 %\n",
      "Accuracy of lion  : 21.00 %\n",
      "Accuracy of lizard : 14.00 %\n",
      "Accuracy of lobster : 8.00 %\n",
      "Accuracy of man   : 13.00 %\n",
      "Accuracy of maple_tree : 22.00 %\n",
      "Accuracy of motorcycle : 38.00 %\n",
      "Accuracy of mountain : 17.00 %\n",
      "Accuracy of mouse : 1.00 %\n",
      "Accuracy of mushroom : 14.00 %\n",
      "Accuracy of oak_tree : 67.00 %\n",
      "Accuracy of orange : 45.00 %\n",
      "Accuracy of orchid : 32.00 %\n",
      "Accuracy of otter : 1.00 %\n",
      "Accuracy of palm_tree : 31.00 %\n",
      "Accuracy of pear  : 17.00 %\n",
      "Accuracy of pickup_truck : 37.00 %\n",
      "Accuracy of pine_tree : 19.00 %\n",
      "Accuracy of plain : 79.00 %\n",
      "Accuracy of plate : 36.00 %\n",
      "Accuracy of poppy : 38.00 %\n",
      "Accuracy of porcupine : 11.00 %\n",
      "Accuracy of possum : 4.00 %\n",
      "Accuracy of rabbit : 4.00 %\n",
      "Accuracy of raccoon : 9.00 %\n",
      "Accuracy of ray   : 17.00 %\n",
      "Accuracy of road  : 53.00 %\n",
      "Accuracy of rocket : 42.00 %\n",
      "Accuracy of rose  : 25.00 %\n",
      "Accuracy of sea   : 34.00 %\n",
      "Accuracy of seal  : 3.00 %\n",
      "Accuracy of shark : 9.00 %\n",
      "Accuracy of shrew : 13.00 %\n",
      "Accuracy of skunk : 36.00 %\n",
      "Accuracy of skyscraper : 45.00 %\n",
      "Accuracy of snail : 17.00 %\n",
      "Accuracy of snake : 13.00 %\n",
      "Accuracy of spider : 15.00 %\n",
      "Accuracy of squirrel : 6.00 %\n",
      "Accuracy of streetcar : 11.00 %\n",
      "Accuracy of sunflower : 54.00 %\n",
      "Accuracy of sweet_pepper : 21.00 %\n",
      "Accuracy of table : 12.00 %\n",
      "Accuracy of tank  : 37.00 %\n",
      "Accuracy of telephone : 31.00 %\n",
      "Accuracy of television : 22.00 %\n",
      "Accuracy of tiger : 10.00 %\n",
      "Accuracy of tractor : 20.00 %\n",
      "Accuracy of train : 17.00 %\n",
      "Accuracy of trout : 23.00 %\n",
      "Accuracy of tulip : 8.00 %\n",
      "Accuracy of turtle : 14.00 %\n",
      "Accuracy of wardrobe : 53.00 %\n",
      "Accuracy of whale : 38.00 %\n",
      "Accuracy of willow_tree : 28.00 %\n",
      "Accuracy of wolf  : 19.00 %\n",
      "Accuracy of woman : 4.00 %\n",
      "Accuracy of worm  : 11.00 %\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = create_optimizer(mlp, learning_rate=0.001)\n",
    "\n",
    "train(mlp, train_loader, criterion, optimizer, epochs=10)\n",
    "validate(mlp, test_loader)\n",
    "validate_per_class(mlp, test_loader, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train the ConvNet model on the CIFAR-100 dataset using the training data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.5333\n",
      "Epoch [2/10], Loss: 2.9029\n",
      "Epoch [3/10], Loss: 2.5651\n",
      "Epoch [4/10], Loss: 2.3241\n",
      "Epoch [5/10], Loss: 2.1014\n",
      "Epoch [6/10], Loss: 1.8876\n",
      "Epoch [7/10], Loss: 1.6992\n",
      "Epoch [8/10], Loss: 1.5163\n",
      "Epoch [9/10], Loss: 1.3262\n",
      "Epoch [10/10], Loss: 1.1518\n",
      "Finished Training\n",
      "Accuracy of the network on the test images: 35.10 %\n",
      "Accuracy of apple : 60.00 %\n",
      "Accuracy of aquarium_fish : 48.00 %\n",
      "Accuracy of baby  : 33.00 %\n",
      "Accuracy of bear  : 13.00 %\n",
      "Accuracy of beaver : 21.00 %\n",
      "Accuracy of bed   : 34.00 %\n",
      "Accuracy of bee   : 39.00 %\n",
      "Accuracy of beetle : 38.00 %\n",
      "Accuracy of bicycle : 47.00 %\n",
      "Accuracy of bottle : 58.00 %\n",
      "Accuracy of bowl  : 19.00 %\n",
      "Accuracy of boy   : 16.00 %\n",
      "Accuracy of bridge : 43.00 %\n",
      "Accuracy of bus   : 31.00 %\n",
      "Accuracy of butterfly : 28.00 %\n",
      "Accuracy of camel : 24.00 %\n",
      "Accuracy of can   : 37.00 %\n",
      "Accuracy of castle : 63.00 %\n",
      "Accuracy of caterpillar : 21.00 %\n",
      "Accuracy of cattle : 30.00 %\n",
      "Accuracy of chair : 65.00 %\n",
      "Accuracy of chimpanzee : 44.00 %\n",
      "Accuracy of clock : 28.00 %\n",
      "Accuracy of cloud : 39.00 %\n",
      "Accuracy of cockroach : 58.00 %\n",
      "Accuracy of couch : 29.00 %\n",
      "Accuracy of crab  : 32.00 %\n",
      "Accuracy of crocodile : 30.00 %\n",
      "Accuracy of cup   : 50.00 %\n",
      "Accuracy of dinosaur : 27.00 %\n",
      "Accuracy of dolphin : 33.00 %\n",
      "Accuracy of elephant : 28.00 %\n",
      "Accuracy of flatfish : 31.00 %\n",
      "Accuracy of forest : 34.00 %\n",
      "Accuracy of fox   : 34.00 %\n",
      "Accuracy of girl  : 10.00 %\n",
      "Accuracy of hamster : 41.00 %\n",
      "Accuracy of house : 23.00 %\n",
      "Accuracy of kangaroo : 19.00 %\n",
      "Accuracy of keyboard : 51.00 %\n",
      "Accuracy of lamp  : 27.00 %\n",
      "Accuracy of lawn_mower : 54.00 %\n",
      "Accuracy of leopard : 30.00 %\n",
      "Accuracy of lion  : 38.00 %\n",
      "Accuracy of lizard : 23.00 %\n",
      "Accuracy of lobster : 14.00 %\n",
      "Accuracy of man   : 16.00 %\n",
      "Accuracy of maple_tree : 60.00 %\n",
      "Accuracy of motorcycle : 55.00 %\n",
      "Accuracy of mountain : 55.00 %\n",
      "Accuracy of mouse : 16.00 %\n",
      "Accuracy of mushroom : 30.00 %\n",
      "Accuracy of oak_tree : 44.00 %\n",
      "Accuracy of orange : 54.00 %\n",
      "Accuracy of orchid : 34.00 %\n",
      "Accuracy of otter : 6.00 %\n",
      "Accuracy of palm_tree : 43.00 %\n",
      "Accuracy of pear  : 37.00 %\n",
      "Accuracy of pickup_truck : 31.00 %\n",
      "Accuracy of pine_tree : 29.00 %\n",
      "Accuracy of plain : 62.00 %\n",
      "Accuracy of plate : 39.00 %\n",
      "Accuracy of poppy : 53.00 %\n",
      "Accuracy of porcupine : 38.00 %\n",
      "Accuracy of possum : 5.00 %\n",
      "Accuracy of rabbit : 14.00 %\n",
      "Accuracy of raccoon : 20.00 %\n",
      "Accuracy of ray   : 37.00 %\n",
      "Accuracy of road  : 64.00 %\n",
      "Accuracy of rocket : 58.00 %\n",
      "Accuracy of rose  : 52.00 %\n",
      "Accuracy of sea   : 56.00 %\n",
      "Accuracy of seal  : 7.00 %\n",
      "Accuracy of shark : 24.00 %\n",
      "Accuracy of shrew : 26.00 %\n",
      "Accuracy of skunk : 51.00 %\n",
      "Accuracy of skyscraper : 66.00 %\n",
      "Accuracy of snail : 18.00 %\n",
      "Accuracy of snake : 12.00 %\n",
      "Accuracy of spider : 42.00 %\n",
      "Accuracy of squirrel : 12.00 %\n",
      "Accuracy of streetcar : 37.00 %\n",
      "Accuracy of sunflower : 57.00 %\n",
      "Accuracy of sweet_pepper : 26.00 %\n",
      "Accuracy of table : 37.00 %\n",
      "Accuracy of tank  : 36.00 %\n",
      "Accuracy of telephone : 39.00 %\n",
      "Accuracy of television : 40.00 %\n",
      "Accuracy of tiger : 23.00 %\n",
      "Accuracy of tractor : 33.00 %\n",
      "Accuracy of train : 21.00 %\n",
      "Accuracy of trout : 40.00 %\n",
      "Accuracy of tulip : 18.00 %\n",
      "Accuracy of turtle : 15.00 %\n",
      "Accuracy of wardrobe : 73.00 %\n",
      "Accuracy of whale : 42.00 %\n",
      "Accuracy of willow_tree : 31.00 %\n",
      "Accuracy of wolf  : 40.00 %\n",
      "Accuracy of woman : 18.00 %\n",
      "Accuracy of worm  : 23.00 %\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = create_optimizer(convnet, learning_rate=0.001)\n",
    "\n",
    "train(convnet, train_loader, criterion, optimizer, epochs=10)\n",
    "validate(convnet, test_loader)\n",
    "validate_per_class(convnet, test_loader, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-7\"></a>\n",
    "### **Section 7: Setting Up the Hyperparameters (14 points)**\n",
    "\n",
    "In this section, you will experiment with both the `ConvNet` and `TwoLayerNet` models by setting up and tuning the hyperparameters to achieve the highest possible accuracy. You have the flexibility to modify the training process, including the `train` function, `DataLoader`, `transform` functions, and optimizer as needed.\n",
    "\n",
    "1. Adjust the hyperparameters, including learning rate, batch size, number of epochs, optimizer, weight decay, and transform function to improve the performance of both networks. Modify the training procedure and architecture as necessary. You can also add components like Batch Normalization layers.\n",
    "2. Add two more layers to both `TwoLayerNet` and `ConvNet`. You can decide the size and placement of these layers. Evaluate if these changes result in higher performance and explain your findings.\n",
    "3. Show the final results and describe the modifications made to enhance performance. Discuss the impact of hyperparameter tuning on both `TwoLayerNet` and `ConvNet`.\n",
    "4. Compare the two networks in terms of architecture, performance, and learning rates. Provide a detailed explanation of the differences observed.\n",
    "\n",
    "**Note:** Do not use external pre-trained networks and limit additional convolutional layers to a maximum of three beyond the original architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 100\u001b[0m\n\u001b[1;32m     96\u001b[0m         probas \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m probas\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(trial):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# hyperparameters to tune\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "def create_transforms_with_data_augmentation():\n",
    "    '''\n",
    "    Creates the data transformations for the CIFAR-100 dataset.\n",
    "    Includes data augmentation for the training set.\n",
    "    \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: The data transformations for the dataset.\n",
    "    '''\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    \n",
    "    return transform_train, transform_test\n",
    "\n",
    "\n",
    "class FourLayerNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        '''\n",
    "        Initializes the two-layer neural network model.\n",
    "        Added extra hidden layers for enhanced capacity.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input features.\n",
    "            hidden_size1 (int): The size of the first hidden layer.\n",
    "            hidden_size2 (int): The size of the second hidden layer.\n",
    "            num_classes (int): The number of classes in the dataset.\n",
    "        '''\n",
    "\n",
    "        super(FourLayerNet, self).__init__()\n",
    "        \n",
    "        self.dense1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.dense2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dense3 = nn.Linear(hidden_size2, hidden_size2 // 2)\n",
    "        self.dense4 = nn.Linear(hidden_size2 // 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        flat_x = torch.flatten(x, start_dim=1)\n",
    "        hidden = F.relu(self.dense1(flat_x))\n",
    "        hidden = F.relu(self.dense2(hidden))\n",
    "        hidden = F.relu(self.dense3(hidden))\n",
    "        output = F.log_softmax(self.dense4(hidden), dim=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class DeeperConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, num_classes=100):\n",
    "        '''\t\n",
    "        Initializes the convolutional neural network model.\n",
    "        Added extra convolutional layers for deeper feature extraction.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels (3 for RGB).\n",
    "            num_classes (int): Number of output classes.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 6 * in_channels, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(6 * in_channels, 16 * in_channels, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16 * in_channels, 32 * in_channels, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32 * in_channels, 64 * in_channels, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 5 * 5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        probas = F.log_softmax(logits, dim=1)\n",
    "        return probas\n",
    "    \n",
    "\n",
    "def objective(trial):\n",
    "    # hyperparameters to tune\n",
    "    learning_rate = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "    weight_decay = trial.suggest_uniform('weight_decay', 0, 0.001)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 100, 500)\n",
    "\n",
    "    model = TwoLayerNet(input_size=3*32*32, hidden_size=hidden_size, num_classes=100) \n",
    "    model.to(device)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    # fixed number of epochs\n",
    "    train(model, train_loader, criterion, optimizer, epochs=10)\n",
    "\n",
    "    # validate and return accuracy\n",
    "    accuracy = validate(model, test_loader)\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1) \n",
    "\n",
    "# best hyperparameters\n",
    "print(f\"Best trial: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the performance of TwoLayerNet after hyperparameter tuning and compare it with the ConvNet model. Provide a detailed explanation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the performance of ConvNet after hyperparameter tuning and compare it with the TwoLayerNet model. Provide a detailed explanation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-8\"></a>\n",
    "### **Section 8: Visualizing the STL-10 Dataset and Preparing the Data Loader (3 points)**\n",
    "\n",
    "In this section, you will work with a subset of the [STL-10](https://cs.stanford.edu/~acoates/stl10/) dataset, containing higher resolution images and different object classes than CIFAR-100. Before fine-tuning your ConvNet on this dataset, first complete the `visualise_stl10` function to display sample images from the following 5 classes:\n",
    "\n",
    "1. **Bird**\n",
    "2. **Deer**\n",
    "3. **Dog**\n",
    "4. **Horse**\n",
    "5. **Monkey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_stl10(class_mapping):\n",
    "    '''\n",
    "    Visualizes 5 images from each specified class in the STL-10 dataset.\n",
    "\n",
    "    Args:\n",
    "        class_mapping (dict): A dictionary mapping class indices to class names.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class mapping for bird, deer, dog, horse, and monkey\n",
    "class_mapping = {1: 'bird', 4: 'deer', 5: 'dog', 6: 'horse', 7: 'monkey'}\n",
    "\n",
    "# Visualize STL-10 classes\n",
    "visualise_stl10(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After visualizing the data, implement the `STL10_loader` class to create a custom data loader that initializes the dataset, extracts the target classes, and applies the necessary image transformations. Once these tasks are completed, you will move on to fine-tuning the ConvNet on this dataset in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STL10_loader(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        '''\n",
    "        Initializes the STL10 dataset.\n",
    "\n",
    "        Args:\n",
    "            root (str): Root directory of the dataset.\n",
    "            train (bool): If True, use the training set, otherwise use the test set.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of samples in the dataset.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Retrieves a sample from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the transformed image and its target label.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-9\"></a>\n",
    "### **Section 9: Fine-tuning ConvNet on STL-10 (14 points)**\n",
    "\n",
    "In this section, you will load the pre-trained parameters of the ConvNet (trained on CIFAR-100) and modify the output layer to adapt it to the new dataset containing 5 classes. You can either first load the pre-trained parameters and then modify the output layer, or change the output layer before loading the matched pre-trained parameters. Once modified, you will train the model and document the settings of hyperparameters, accuracy, and learning curve. Additionally, visualize both the training loss and accuracy to assess the learning process. To gain a deeper understanding of the feature learning process, consider using techniques like [**t-sne**](https://lvdmaaten.github.io/tsne/) for feature space visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-10\"></a>\n",
    "### **Section 10: Bonus Challenge (optional)**\n",
    "\n",
    "Try to achieve the highest possible accuracy on the test dataset (5 classes from STL-10) by adjusting hyperparameters, modifying architectures, or applying techniques like data augmentation. The top-performing teams will earn bonus points that can significantly boost their final lab grade, even allowing it to exceed 10 (up to 11):\n",
    "\n",
    "- **1st place:** +1.0 to the final grade of the final lab\n",
    "- **2nd place:** +0.8 to the final grade of the final lab\n",
    "- **3rd place:** +0.6 to the final grade of the final lab\n",
    "- **4th place:** +0.4 to the final grade of the final lab\n",
    "- **5th place:** +0.2 to the final grade of the final lab\n",
    "\n",
    "**Hint:** You may use techniques like data augmentation, freezing early layers, modifying architecture, or optimizing hyperparameters. Only data from CIFAR-100 and STL-10 can be used, and you cannot add more than 3 additional convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-x\"></a>\n",
    "### **Section X: Individual Contribution Report *(Mandatory)***\n",
    "\n",
    "Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | Contribution on Research | Contribution on Programming | Contribution on Writing |\n",
    "| -------- | ------- | ------- | ------- |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - End of Notebook -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv1_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
